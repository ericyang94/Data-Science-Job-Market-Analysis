{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\"\"\"Generate a url from job position and location search result\"\"\"\n",
    "def get_url(position,location):\n",
    "    template = 'https://www.indeed.com/jobs?q=title%3A({})&l={}&jt=fulltime&limit=50'\n",
    "    url = template.format(position,location)\n",
    "    return url\n",
    "\n",
    "\"\"\"Extract job data from a single listing\"\"\"\n",
    "def get_listing(card):\n",
    "    sleeptime = random.randrange(3,6)\n",
    "    time.sleep(sleeptime)\n",
    "    atag = card.h2.a\n",
    "    title = atag.get('title')\n",
    "    company = card.find('span', 'company').text.strip()\n",
    "    job_url = 'https://www.indeed.com/viewjob?jk=' + card.get('data-jk')\n",
    "    response_desc = requests.get(job_url)\n",
    "    soup_desc = BeautifulSoup(response_desc.text, 'html.parser')\n",
    "    description = soup_desc.find('div', 'jobsearch-jobDescriptionText')\n",
    "    if description is not None:\n",
    "        description = description.text.strip().replace('\\n', ' ')\n",
    "    location = soup_desc.find('div', \"jobsearch-InlineCompanyRating\")\n",
    "    if location is not None:\n",
    "        location = location.find_all('div')[-1].text.strip()\n",
    "    listing = (title, company, location, description, job_url)\n",
    "    return listing\n",
    "\n",
    "\"\"\"Run job search and save all results to csv file\"\"\"\n",
    "def main(position, location):\n",
    "    listings = []\n",
    "    url = get_url(position, location)\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        cards = soup.find_all('div', 'jobsearch-SerpJobCard')\n",
    "        \n",
    "        \"\"\"Get all job listings from current search page\"\"\"\n",
    "        for card in cards:\n",
    "            listing = get_listing(card)\n",
    "            listings.append(listing)\n",
    "            \n",
    "        \"\"\"Check if reached last search page\"\"\"\n",
    "        try:\n",
    "            url = 'https://www.indeed.com' + soup.find('a', {'aria-label': 'Next'}).get('href')\n",
    "        except AttributeError:\n",
    "            break\n",
    "    \n",
    "    \"\"\"Create a csv file with our search results\"\"\"\n",
    "    csv_title = 'listings_{}.csv'.format(location.replace(\" \",\"\"))\n",
    "    with open(csv_title, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Title', 'Company', 'Location', 'Description', 'URL'])\n",
    "        writer.writerows(listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "main('data scientist','united states')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
